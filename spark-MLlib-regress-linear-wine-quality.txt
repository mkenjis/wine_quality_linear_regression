 
val rdd1 = sc.textFile("wine_quality/winequality-red.csv").filter( x => ! x.contains("fixed acidity"))
 
val rdd = rdd1.map(x => x.split(";")).map(x => x.map(y => y.toDouble))

rdd.take(5)
res0: Array[Array[Double]] = Array(Array(7.4, 0.7, 0.0, 1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4, 5.0), Array(7.8, 0.88, 0.0, 2.6, 0.098, 25.0, 67.0, 0.9968, 3.2, 0.68, 9.8, 5.0), Array(7.8, 0.76, 0.04, 2.3, 0.092, 15.0, 54.0, 0.997, 3.26, 0.65, 9.8, 5.0), Array(11.2, 0.28, 0.56, 1.9, 0.075, 17.0, 60.0, 0.998, 3.16, 0.58, 9.8, 6.0), Array(7.4, 0.7, 0.0, 1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4, 5.0))

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = rdd.map( x => {
  val arr_size = x.size - 1
  val l = x(arr_size).toInt
  val f = x.slice(0,arr_size)
  LabeledPoint(l,Vectors.dense(f))
})

data.cache

val sets = data.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

---- MLlib Linear regression --------------

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)

---- step size = 0.1
alg.optimizer.setStepSize(0.1)
val model = alg.run(trainSet)
// model: intercept = NaN, numFeatures = 11

---- step size = 0.01
alg.optimizer.setStepSize(0.01)
val model = alg.run(trainSet)
// model: intercept = -1.5148561670934835E93, numFeatures = 11

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)  // predict values are horrible
// res10: Array[(Double, Double)] = Array((-4.031809281483744E96,5.0), (-7.8033337104762965E96,5.0), (-2.8568935121575425E96,7.0), (-3.4649905230341324E96,5.0), (-4.30658416486107E96,5.0), (-7.2973178031654645E96,5.0), (-4.30658416486107E96,5.0), (-4.223356104325369E96,5.0), (-1.8979602534302264E96,6.0), (-9.419837808072497E96,5.0))

---- step size = 0.001
alg.optimizer.setStepSize(0.001)
val model = alg.run(trainSet)
model:  intercept = 1.013746015606052, numFeatures = 11

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
// res12: Array[(Double, Double)] = Array((4.458003028909236,5.0), (5.6163104541094375,5.0), (4.566533545238184,7.0), (4.297706068827487,5.0), (4.493486479799625,5.0), (4.8394154226166615,5.0), (4.493486479799625,5.0), (4.578668157347532,5.0), (4.131612027143722,6.0), (5.343072951870064,5.0))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 1.1803671866946677
validMetrics.meanSquaredError  // 1.3932666954254844

---- Estimation is not so bad. But analyze the individual statistics and standardize 

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = data.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res4: org.apache.spark.mllib.linalg.Vector = [15.9,1.58,1.0,15.5,0.611,72.0,289.0,1.00369,4.01,2.0,14.9]

matrixSummary.min
res5: org.apache.spark.mllib.linalg.Vector = [4.6,0.12,0.0,0.9,0.012,1.0,6.0,0.99007,2.74,0.33,8.4]

matrixSummary.mean
res6: org.apache.spark.mllib.linalg.Vector = [8.319637273295799,0.5278205128205145,0.27097560975609764,2.538805503439649,0.08746654158849286,15.874921826141357,46.46779237023142,0.9967466791744843,3.3111131957473443,0.6581488430268921,10.422983114446522]

matrixSummary.variance
res7: org.apache.spark.mllib.linalg.Vector = [3.0314163889978145,0.03206237765155158,0.03794748313440572,1.9878971329859645,0.0022151426533009895,109.41488383305862,1082.102372532582,3.562029453327622E-6,0.023835180545412823,0.028732616129762,1.135647395000471]


---- Calculate the column similarity matrix

import org.apache.spark.mllib.linalg.Vectors

val vect = rdd.map( x => Vectors.dense(x))

import org.apache.spark.mllib.linalg.distributed.RowMatrix
val wineMat = new RowMatrix(vect)

val colsims = wineMat.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    print(f"$sim%.4f ")
    i = x._1._1
  } else print(f"$sim%.4f ")
})


0.9101 0.8751 0.8672 0.8711 0.8001 0.7856 0.9791 0.9712 0.9572 0.9725 0.9725   "fixed acidity" x quality
       0.6655 0.8283 0.8433 0.7890 0.7872 0.9470 0.9495 0.8962 0.9355 0.9197   "volatile acidity" x quality
              0.7506 0.7715 0.6586 0.6748 0.8125 0.7964 0.8319 0.8144 0.8226   "citric acid" x quality
                     0.7828 0.7801 0.7706 0.8746 0.8714 0.8474 0.8718 0.8664   "residual sugar"  x quality
                            0.7369 0.7318 0.8808 0.8739 0.8967 0.8655 0.8631   "chlorides" x quality
                                   0.8938 0.8351 0.8360 0.8158 0.8269 0.8227   "free sulfur dioxide" x quality
                                          0.8163 0.8136 0.7967 0.8000 0.7929   "total sulfur dioxide" x quality
                                                 0.9989 0.9685 0.9947 0.9898   "density" x quality
                                                        0.9651 0.9947 0.9884   "pH" x quality
                                                               0.9658 0.9675   "sulphates" x quality
                                                                      0.9916   "alcohol" x "quality"

---- In the next analysis, it will consider only "fixed acidity", "volatile acidity", "density", "pH", "sulphates","alcohol" to quality		  
																	  
val data = rdd.map( r => {
  val arr_size = r.size
  val l = r(arr_size-1).toInt
  val f = Array(r(0),r(1),r(7),r(8),r(9),r(10))
  LabeledPoint(l,Vectors.dense(f))
})

data.cache

val sets = data.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)

---- step size = 0.1
alg.optimizer.setStepSize(0.1)
val model = alg.run(trainSet)
model: intercept = 9.281747011595764E13, numFeatures = 6

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)  // predict values are horrible
res13: Array[(Double, Double)] = Array((2.9916027833052085E15,5.0), (5.2083011004994825E14,5.0), (2.1721137212308112E15,5.0), (2.533426126205943E15,5.0), (2.400076857190313E15,5.0), (8.710356762010516E14,5.0), (2.7133221381754655E15,6.0), (1.4703574899636862E14,5.0), (1.5140524444303712E15,5.0), (1.9035731951854078E15,5.0))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 2.0014926325619645E15
validMetrics.meanSquaredError  // 4.0059727581998234E30

---- step size = 0.01
alg.optimizer.setStepSize(0.01)
val model = alg.run(trainSet)
model: intercept = 1.0244668493496212, numFeatures = 6

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)  
res17: Array[(Double, Double)] = Array((5.2033281092142305,5.0), (5.361820963442839,5.0), (4.999169971638653,5.0), (5.000866870748702,5.0), (4.953520520656063,5.0), (5.241740023457866,5.0), (5.102976560802815,6.0), (5.459981583793877,5.0), (5.447795322054186,5.0), (5.270471818790001,5.0))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 0.6853457086642035
validMetrics.meanSquaredError  // 0.4696987403844393


-------------------------- Decide to scale features because columns have different scales and then retrain the model 

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, true).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))
val validScaled = testSet.map(x => LabeledPoint(x.label, scaler.transform(x.features)))

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)

---- step size = 0.1
alg.optimizer.setStepSize(0.1)
val model = alg.run(trainScaled)
// model: intercept = 5.074932507637341, numFeatures = 11

val validPredicts = validScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
// res26: Array[(Double, Double)] = Array((4.491593569763872,5.0), (4.508869707256585,5.0), (4.777147437738173,7.0), (5.506683076704168,5.0), (5.169177233080635,5.0), (4.669323069983979,5.0), (5.169177233080635,5.0), (4.489842662974049,5.0), (5.031715598882036,6.0), (5.214509262137191,5.0))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 0.8655612981495765
validMetrics.meanSquaredError  // 0.7491963608543801

---- step size = 1.0 -- Chosen model due to minimum variability
alg.optimizer.setStepSize(1.0)
val model = alg.run(trainScaled)
// model: intercept = 5.634945397815972, numFeatures = 11

val validPredicts = validScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
//res30: Array[(Double, Double)] = Array((5.040734539026857,5.0), (5.112972722657878,5.0), (5.342588343196543,7.0), (6.1483796220973,5.0), (5.762648438562308,5.0), (5.2170016368436105,5.0), (5.762648438562308,5.0), (5.042503396728593,5.0), (5.5932953197796556,6.0), (5.762100380642349,5.0))

val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 0.6663475541589079
validMetrics.meanSquaredError  // 0.4440190629335587
